{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "__title__ = 'PUBMED-Query search'\n",
    "__version__ = '1.0.0'\n",
    "__author__ = 'Chirag Gupta'\n",
    "\n",
    "\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import lxml.html.clean\n",
    "import requests\n",
    "\n",
    "#making csv file to record data\n",
    "csv_out = open('NeuroDis.csv', 'ab')\n",
    "csv_out.truncate(0)\n",
    "writer = csv.writer(csv_out, dialect='excel', delimiter=',', encoding='utf-8')\n",
    "\n",
    "writer.writerow(['Abstract'])\n",
    "#indes: serial number of result\n",
    "index=0\n",
    "\n",
    "#there are 10 results in each page, so we need to go through 10 pages to get 100 search results\n",
    "i=['1','2','3','4','5','6','7','8','9','10']\n",
    "for x in i:\n",
    "    Search_Term = 'neurodegenerative%20diseases&page='\n",
    "    Search_URL = 'https://www.ncbi.nlm.nih.gov/pubmed?term=' + Search_Term + x #To fetch more results than the default 20, add the parameter dispmax=## to the URL, e.g. https://www.ncbi.nlm.nih.gov/pubmed?term=((histone)%20AND%20chromatin)%20AND%20ESC&dispmax=100\n",
    "#     print(Search_URL)\n",
    "    #query to scrap search result page\n",
    "    Search_Page = requests.get(Search_URL)\n",
    "    Tree = html.fromstring(Search_Page.content)\n",
    "    #making xpath using lxml\n",
    "    Id=Tree.xpath('//meta[@name=\"log_displayeduids\"]/@content')[0]\n",
    "    #now we now all 10 ids of papers present in that page related to our search\n",
    "    Ids=Id.split(\",\")\n",
    "    for Id in Ids:\n",
    "        index=index+1\n",
    "        url= 'https://pubmed.ncbi.nlm.nih.gov/'+Id+'/'\n",
    "#         print(Id)\n",
    "        Page=requests.get(url)\n",
    "        tree=html.fromstring(Page.content)\n",
    "        abstracts= tree.xpath('//div[@class=\"article-page\"]/main[@class=\"article-details\"]/div[@class=\"abstract\"]/div[@class=\"abstract-content selected\"]')\n",
    "        for abstract in abstracts:\n",
    "            #if there are more than one headings, it will join all such lists with whitespace\n",
    "            s=' '.join(abstract.text_content().split())\n",
    "        if(len(abstracts)>0):    \n",
    "            writer.writerow([s])\n",
    "        else:\n",
    "            #'N' means there is no abstract corresponding to that search\n",
    "            writer.writerow(['N'])\n",
    "#         print(s)\n",
    "csv_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use saved data as array for further computations\n",
    "import pandas as pd\n",
    "data=pd.read_csv('NeuroDis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Print- Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurodegenerative disorders are characterized by progressive loss of selectively vulnerable populations of neurons, which contrasts with select static neuronal loss because of metabolic or toxic disorders. Neurodegenerative diseases can be classified according to primary clinical features (e.g., dementia, parkinsonism, or motor neuron disease), anatomic distribution of neurodegeneration (e.g., frontotemporal degenerations, extrapyramidal disorders, or spinocerebellar degenerations), or principal molecular abnormality. The most common neurodegenerative disorders are amyloidoses, tauopathies, α-synucleinopathies, and TDP-43 proteinopathies. The protein abnormalities in these disorders have abnormal conformational properties. Growing experimental evidence suggests that abnormal protein conformers may spread from cell to cell along anatomically connected pathways, which may in part explain the specific anatomical patterns observed at autopsy. In this review, we detail the human pathology of select neurodegenerative disorders, focusing on their main protein aggregates.\n",
      "\n",
      "Many lines of evidence suggest that mitochondria have a central role in ageing-related neurodegenerative diseases. Mitochondria are critical regulators of cell death, a key feature of neurodegeneration. Mutations in mitochondrial DNA and oxidative stress both contribute to ageing, which is the greatest risk factor for neurodegenerative diseases. In all major examples of these diseases there is strong evidence that mitochondrial dysfunction occurs early and acts causally in disease pathogenesis. Moreover, an impressive number of disease-specific proteins interact with mitochondria. Thus, therapies targeting basic mitochondrial processes, such as energy metabolism or free-radical generation, or specific interactions of disease-related proteins with mitochondria, hold great promise.\n",
      "\n",
      "%%Alert%%: reaseach paper corresponding to this search result dont have any abstract\n",
      "\n",
      "One of the most striking hallmarks shared by various neurodegenerative diseases, including Parkinson's disease, Alzheimer's disease (AD), and amyotrophic lateral sclerosis, is microglia-mediated neuroinflammation. Increasing evidence indicates that microglial activation in the central nervous system is heterogeneous, which can be categorized into two opposite types: M1 phenotype and M2 phenotype. Depending on the phenotypes activated, microglia can produce either cytotoxic or neuroprotective effects. In this review, we focus on the potential role of M1 and M2 microglia and the dynamic changes of M1/M2 phenotypes that are critically associated with the neurodegenerative diseases. Generally, M1 microglia predominate at the injury site at the end stage of disease, when the immunoresolution and repair process of M2 microglia are dampened. This phenotype transformation is very complicated in AD due to the phagocytosis of regionally distributed β-amyloid (Aβ) plaque and tangles that are released into the extracellular space. The endogenous stimuli including aggregated α-synuclein, mutated superoxide dismutase, Aβ, and tau oligomers exist in the milieu that may persistently activate M1 pro-inflammatory responses and finally lead to irreversible neuron loss. The changes of microglial phenotypes depend on the disease stages and severity; mastering the stage-specific switching of M1/M2 phenotypes within appropriate time windows may provide better therapeutic benefit.\n",
      "\n",
      "Clinical gene therapy has made important advances over the last decade. Among neurological diseases, severe genetic neurodegenerative conditions have been the focus of initial clinical applications. Gene therapy has also addressed complex neurodegenerative diseases, particularly Parkinson's disease, with encouraging results in human patients, demonstrating that specific targeting of central nervous system (CNS) cells is a relevant strategy for severe pathologies and that efficient access to the CNS with viral vectors is an achievable goal. The purpose of this review is to summarize the gene therapy clinical applications that have been conducted for neurodegenerative diseases. Limitations and hurdles to obtain and demonstrate benefit in patients, and the new developments that should allow new clinical applications with high beneficial potential are discussed.\n",
      "\n",
      "Neurodegenerative and cerebrovascular diseases cause considerable human suffering, and therapy options for these two disease categories are limited or non-existing. It is an emerging notion that neurodegenerative and cerebrovascular diseases are linked in several ways, and in this review, we discuss the current status regarding vascular dysregulation in neurodegenerative disease, and conversely, how cerebrovascular diseases are associated with central nervous system (CNS) degeneration and dysfunction. The emerging links between neurodegenerative and cerebrovascular diseases are reviewed with a particular focus on pericytes-important cells that ensheath the endothelium in the microvasculature and which are pivotal for blood-brain barrier function and cerebral blood flow. Finally, we address how novel molecular and cellular insights into pericytes and other vascular cell types may open new avenues for diagnosis and therapy development for these important diseases.\n",
      "\n",
      "Extracellular vesicles (EVs) are released by all neural cells, including neurons, oligodendrocytes, astrocytes, and microglia. The lack of adequate technology has not halted neuroscientists from investigating EVs as a mean to decipher neurodegenerative disorders, still in search of comprehensible pathogenic mechanisms and efficient treatment. EVs are thought to be one of ways neurodegenerative pathologies spread in the brain, but also one of the ways the brain tries to displace toxic proteins, making their meaning in pathogenesis uncertain. EVs, however do reach biological fluids where they can be analyzed, and might therefore constitute clinically decisive biomarkers for neurodegenerative diseases in the future. Finally, if they constitute a physiological inter-cell communication system, they may represent also a very specific drug delivery tool for a difficult target such as the brain. We try to resume here available information on the role of EVs in neurodegeneration, with a special focus on Alzheimer's disease, progressive multiple sclerosis, amyotrophic lateral sclerosis, and Huntington's disease.\n",
      "\n",
      "Neurodegenerative diseases are sporadic and rare hereditary disorders of the central nervous system, which cause a slowly progressive loss of function of specific neuron populations and their connections. Severe impairments and care dependency can be the sequelae. Neurodegenerative disorders are diseases of older people; therefore, the demographic shift leads to an increase in the number of affected patients. Radiologists will also become more involved. For this reason important neurodegenerative diseases are presented in this article. In addition to Alzheimer's and Parkinson's diseases these also include frontotemporal lobar degeneration, Lewy body dementia, vascular dementia, Creutzfeldt-Jakob disease and Huntington's chorea. The clinical symptoms and diagnostics are described, whereby the focus lies on typical results of morphological imaging.\n",
      "\n",
      "Neuronal cell death in the central nervous system has always been a challenging process to decipher. In normal physiological conditions, neuronal cell death is restricted in the adult brain, even in aged individuals. However, in the pathological conditions of various neurodegenerative diseases, cell death and shrinkage in a specific region of the brain represent a fundamental pathological feature across different neurodegenerative diseases. In this review, we will briefly go through the general pathways of cell death and describe evidence for cell death in the context of individual common neurodegenerative diseases, discussing our current understanding of cell death by connecting with renowned pathogenic proteins, including Tau, amyloid-beta, alpha-synuclein, huntingtin and TDP-43.\n",
      "\n",
      "Neurodegenerative diseases are a growing health concern. The increasing incidences of these disorders have a great impact on the patients' quality of life. Although the mechanisms of neurodegenerative diseases are still far from being clarified, several studies look for new discoveries about their pathophysiology and prevention. Furthermore, evidence has shown a strong correlation between obesity and the development of Alzheimer's disease (AD) and Parkinson's disease (PD). Metabolic changes caused by overweight are related to damage to the central nervous system (CNS), which can lead to neural death, either by apoptosis or cell necrosis, as well as alter the synaptic plasticity of the neuron. This review aims to show the association between neurodegenerative diseases, focusing on AD and PD, and metabolic alterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data.head(5)\n",
    "i=0\n",
    "#some abstracts are not given in the site, they are part of 100 search results but we wont print them as first 10 abstracts\n",
    "while (i<10):\n",
    "    if(data['Abstract'][i]!='N'):\n",
    "        print(data['Abstract'][i],end='\\n\\n')\n",
    "    else:\n",
    "        print(\"%%Alert%%: reaseach paper corresponding to this search result dont have any abstract\\n\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special characters are not a word, so this step will remove comma, fullstop, etc. from the word corpus\n",
    "import re\n",
    "s=''\n",
    "for i in range (100):\n",
    "    data['Abstract'][i]=re.sub('[^A-Za-z0-9 ]+', '',data['Abstract'][i])\n",
    "    if(data['Abstract'][i]!='N'):\n",
    "        s=s+' '+data['Abstract'][i]\n",
    "    else:\n",
    "        data['Abstract'][i]=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = re.sub('[^A-Za-z0-9 ]+', '', s)\n",
    "#calculate frequency of word using nltk library\n",
    "from nltk.book import FreqDist\n",
    "#newlist is list of all words\n",
    "newlist=s.split()\n",
    "#normalize capitals- The and the are same\n",
    "for i in range(len(newlist)):\n",
    "    newlist[i]=newlist[i].lower()\n",
    "fdist=FreqDist(newlist)\n",
    "#extracting most common i.e. most frequent words\n",
    "remlist=fdist.most_common(10)\n",
    "# print(remlist)\n",
    "rem=remlist\n",
    "for i in range(0,10):\n",
    "    rem[i]=remlist[i][0]\n",
    "# print(rem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'and', 'in', 'diseases', 'to', 'neurodegenerative', 'a', 'disease', 'for']\n"
     ]
    }
   ],
   "source": [
    "#this will remove frequent 10 words from whole corpus\n",
    "newlist=[w for w in newlist if w.lower() not in rem]\n",
    "\n",
    "#removal from each asbtract \n",
    "for i in range(100):\n",
    "    #split string to list\n",
    "    x=data['Abstract'][i].split()\n",
    "    #remove\n",
    "    y=[w for w in x if w.lower() not in rem]\n",
    "    #list to string\n",
    "    data['Abstract'][i]=' '.join(y)\n",
    "#print removed words\n",
    "print(rem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All abstract are cleansed: Free from most frequently occuring word\n",
    "Last printed values are lowercased words which are removed from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dimensions of each vector is equal to number of words in whole corpus i.e len(fdist)-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list of words, new fdist calculation\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#using numpy for 2D vector where each row vector is vector corresponding to an abstract\n",
    "fdist=FreqDist(newlist)\n",
    "fcom=fdist.most_common(len(fdist))\n",
    "gcom=fcom\n",
    "#gcom is list of words in whole corpus- no of words are number of components of each vector\n",
    "for om in range(len(fdist)):\n",
    "    gcom[om]=fcom[om][0]\n",
    "# print(len(gcom))\n",
    "del fdist,fcom\n",
    "#vector initialization\n",
    "vectors=np.zeros((100,len(gcom)))\n",
    "vectors=np.int_(vectors)\n",
    "\n",
    "#calculate frequency of each word in the given abstract\n",
    "for i in range(0,100):\n",
    "    x=data['Abstract'][i].split()\n",
    "    for q in range(len(x)):\n",
    "        x[q]=x[q].lower()\n",
    "    mdist=FreqDist(x)\n",
    "    com=mdist.most_common(len(mdist))\n",
    "#     print(com)\n",
    "    wcom=[]\n",
    "    for wx in range(len(com)):\n",
    "        wcom.insert(0,com[wx][0])\n",
    "    q=0\n",
    "    for ind in gcom:\n",
    "#         print(ind,end=\"  \")\n",
    "        w=0\n",
    "        if ind in wcom:\n",
    "            for w in range(0,len(com)):\n",
    "                if com[w][0]==ind:\n",
    "                    vectors[i][q]=com[w][1]\n",
    "                    break\n",
    "        else:\n",
    "            #the word is not in this abstract\n",
    "            vectors[i][q]=0\n",
    "        q=q+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 3- Printing first two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 1 1 1 1 1 1 1 6 4 0 0 1 3 0 0 2 1 0 1 0 0 2 0 2 0 1 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2\n",
      " 1 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "[1 2 2 2 1 0 0 1 1 0 0 0 0 2 1 2 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#vectors for first two rows\n",
    "print(vectors[0],end=\"\\n\\n\")\n",
    "print(vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm function to help cosine similarity function\n",
    "def norm(vector):\n",
    "    c=0\n",
    "    for i in range (len(vectors[0])):\n",
    "        c=c+ (vectors[0][i])*(vectors[0][i])\n",
    "    c=pow(c,0.5)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity function\n",
    "def cosine(a, b):\n",
    "    #if both vectors are unequal\n",
    "    for i in range(len(a)):\n",
    "        if(a[i]!=b[i]):\n",
    "            return (np.dot(a,b))/(norm(a)*norm(b))\n",
    "    #if equal\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of similarity matrix\n",
    "sim_mat=np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        sim_mat[i][j]=cosine(vectors[i],vectors[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster function, calculate max similarity centroid and add itself into that cluster\n",
    "def cluster():\n",
    "    for i in list1:\n",
    "        c=-1\n",
    "        q=-1\n",
    "        for j in range(len(lists)):\n",
    "            d=sim_mat[i][lists[j][0]]\n",
    "            if(d>c):\n",
    "                c=d\n",
    "                q=j\n",
    "        lists[q].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new centroid calculation\n",
    "def centroid():\n",
    "    for i in lists:\n",
    "        c=0\n",
    "        for j in range(1,len(i)):\n",
    "            c=c+i[j]\n",
    "        c=c/(len(i)-1)\n",
    "        d=200\n",
    "        k=0\n",
    "        for j in range(1,len(i)):\n",
    "            if(abs(i[j]-c)<d):\n",
    "                d=abs(i[j]-c)\n",
    "                k=j\n",
    "        i[0]=i[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if new clusters are repeating\n",
    "def check(lists,lists2):\n",
    "    l1=[]\n",
    "    l2=[]\n",
    "    if(len(lists)!=len(lists2)):\n",
    "        return 0\n",
    "    for y in range(len(lists)):\n",
    "        l3=[]\n",
    "        l4=[]\n",
    "        #first element of each list is centroid\n",
    "        for z in range(1,len(lists[y])):\n",
    "            l3.append(lists[y][z])\n",
    "        l1.append(l3)\n",
    "        for z in range(1,len(lists2[y])):\n",
    "            l4.append(lists2[y][z])\n",
    "        l2.append(l4)\n",
    "        \n",
    "    l1.sort()\n",
    "    l2.sort()\n",
    "    return (l1==l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Clusters successfully created.\n",
      "Clusters are-\n",
      "\n",
      "[[49, 4, 5, 7, 49, 92], [47, 0, 1, 3, 6, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 40, 41, 43, 44, 45, 47, 50, 51, 52, 53, 54, 55, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 71, 73, 74, 76, 78, 79, 81, 82, 83, 84, 85, 87, 88, 89, 90, 93, 94, 95, 96, 97], [57, 57], [48, 8, 12, 48, 61, 98, 99], [56, 2, 34, 37, 42, 46, 56, 69, 70, 72, 75, 77, 86, 91], [80, 80]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#Generate 6 random numbers between 0 and 99\n",
    "randomlist = random.sample(range(0, 100), 6)\n",
    "\n",
    "lists=[]\n",
    "for i in range(6):\n",
    "    lists.append([randomlist[i]])\n",
    "\n",
    "list1=range(100)\n",
    "lists2=[[]]\n",
    "u=0\n",
    "while(u<10000):\n",
    "    cluster()\n",
    "#     print(lists,end=\"\\n\")\n",
    "    centroid()\n",
    "    c=0\n",
    "    #check if clusters are same as before\n",
    "    if(check(lists,lists2)):\n",
    "        break\n",
    "    lists2=lists\n",
    "#     print(lists,end=\"\\n\\n\")\n",
    "    lists=[]\n",
    "    for i in lists2:\n",
    "        lists.append([i[0]])\n",
    "    u=u+1\n",
    "print(\"6 Clusters successfully created.\\nClusters are-\\n\")\n",
    "print(lists2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#every list of lists is index of abstract which lie in same cluster\n",
    "<br>Next task is to find 3 unique number corresponding to each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is done by adding all the corresponding vectors which lies in same cluster(identified from lists)\n",
    "#6 vectors will come corresponding to each cluster, where ith element of the vector will tell frequency of ith word in that cluster\n",
    "clus_word=[]\n",
    "for i in lists2:\n",
    "    c=vectors[3]\n",
    "    for j in range(1,len(i)):\n",
    "        c=c+vectors[i[j]]\n",
    "#         print(c)\n",
    "    clus_word.append(c)\n",
    "tot=clus_word[0]+clus_word[2]+clus_word[3]+clus_word[4]+clus_word[5]+clus_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding index of word to frequency which will help us during sorting\n",
    "y=[]\n",
    "for i in clus_word:\n",
    "    x=[]\n",
    "    for j in range(len(i)):\n",
    "        x.append([i[j],[tot[j],j]])\n",
    "    x.sort(key=lambda z:z[1][0])\n",
    "    x.sort()\n",
    "    y.append(x)\n",
    "#sorted such that if less frquency in cluster- comes first, same frequency in cluster- one with less frequency in corpus comes first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1 : Most unique numbers are- conclusions ,  deepening  and  appeared\n",
      "Cluster  2 : Most unique numbers are- contribute ,  risk  and  amyloid\n",
      "Cluster  3 : Most unique numbers are- predictive ,  value  and  accuracy\n",
      "Cluster  4 : Most unique numbers are- classical ,  enter  and  exact\n",
      "Cluster  5 : Most unique numbers are- variable ,  powerful  and  agedependent\n",
      "Cluster  6 : Most unique numbers are- grounds ,  discovering  and  compound\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y)):\n",
    "    j=0\n",
    "    while(j<len(y[i])):\n",
    "        if(y[i][j][0]>0):\n",
    "            break\n",
    "        j=j+1\n",
    "    if(j==len(y[i])):\n",
    "        print(\"Cluster \",i+1,\": No unique number, all vectors in this cluster are 0 i.e Abstracts corresponding to this cluster are absent\",end=\"\\n\")\n",
    "    else:\n",
    "        print(\"Cluster \",i+1,\": Most unique numbers are-\",gcom[j],\", \",gcom[j+1],\" and \",gcom[j+2],end=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
